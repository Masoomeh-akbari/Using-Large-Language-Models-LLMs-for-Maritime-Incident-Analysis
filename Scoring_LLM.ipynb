{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Method_number=7\n",
        "#1:WebBert Similarity\n",
        "#2:cosine similarity\n",
        "#3:jaccard similarity\n",
        "#4:bm25 similarity\n",
        "#5:Sørensen-Dice coefficient\n",
        "#6:Levenshtein Similarity\n",
        "#7:Combination by above methods\n",
        "#8:BLEU score\n",
        "#9:rouge score\n",
        "#10:iou\n",
        "#11:Exact Match\n",
        "\n",
        "output_file_name='extracted_data.xlsx'#change it to your file name, and don't forget to put it in the same folder with this program"
      ],
      "metadata": {
        "id": "RHAhbAvEmIfg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "#package required for keywords extraction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rGemeEnnX24",
        "outputId": "a2950d16-862b-4b5a-85ba-db71ea286586"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install semantic-text-similarity\n",
        "!pip install Levenshtein\n",
        "!pip install rank_bm25\n",
        "!pip install nltk\n",
        "!pip install rouge-score\n",
        "#package required for similarity methods"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHz_LPP1bWHF",
        "outputId": "1c9adf31-cc08-4541-a23b-90e04e828936"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting semantic-text-similarity\n",
            "  Downloading semantic_text_similarity-1.0.3-py3-none-any.whl (416 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/416.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/416.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.0/416.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from semantic-text-similarity) (2.3.0+cu121)\n",
            "Collecting strsim (from semantic-text-similarity)\n",
            "  Downloading strsim-0.0.3-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fuzzywuzzy[speedup] (from semantic-text-similarity)\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pytorch-transformers==1.1.0 (from semantic-text-similarity)\n",
            "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from semantic-text-similarity) (1.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (1.25.2)\n",
            "Collecting boto3 (from pytorch-transformers==1.1.0->semantic-text-similarity)\n",
            "  Downloading boto3-1.34.127-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (2024.5.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers==1.1.0->semantic-text-similarity) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->semantic-text-similarity)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->semantic-text-similarity) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->semantic-text-similarity)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-levenshtein>=0.12 (from fuzzywuzzy[speedup]->semantic-text-similarity)\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.25.1 (from python-levenshtein>=0.12->fuzzywuzzy[speedup]->semantic-text-similarity)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-levenshtein>=0.12->fuzzywuzzy[speedup]->semantic-text-similarity)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.127 (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity)\n",
            "  Downloading botocore-1.34.127-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers==1.1.0->semantic-text-similarity)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->semantic-text-similarity) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-transformers==1.1.0->semantic-text-similarity) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->semantic-text-similarity) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.127->boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.127->boto3->pytorch-transformers==1.1.0->semantic-text-similarity) (1.16.0)\n",
            "Installing collected packages: strsim, fuzzywuzzy, rapidfuzz, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, Levenshtein, botocore, s3transfer, python-levenshtein, nvidia-cusolver-cu12, boto3, pytorch-transformers, semantic-text-similarity\n",
            "Successfully installed Levenshtein-0.25.1 boto3-1.34.127 botocore-1.34.127 fuzzywuzzy-0.18.0 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 python-levenshtein-0.25.1 pytorch-transformers-1.1.0 rapidfuzz-3.9.3 s3transfer-0.10.1 semantic-text-similarity-1.0.3 strsim-0.0.3\n",
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.9.3)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0cdf64b7089a83b7fdec3ef9dceaff68ebaf53626a870e5d343a98239d3647e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from semantic_text_similarity.models import WebBertSimilarity\n",
        "from semantic_text_similarity.models import ClinicalBertSimilarity\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import jieba\n",
        "\n",
        "import Levenshtein as lev\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "41kiAwK4bYaU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "ghILqr8KnR1v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zSEuDUCeIjWW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-qdwWOc-WZw",
        "outputId": "2b86e976-cb69-4e66-e295-a1ea79a98c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_before_colon(input_string):\n",
        "    # Find the position of the first occurrence of ':'\n",
        "    colon_pos = input_string.find(':')\n",
        "\n",
        "    # If a colon is found, remove everything before it including the colon\n",
        "    if colon_pos != -1:\n",
        "        return input_string[colon_pos + 1:].strip()\n",
        "    else:\n",
        "        # If no colon is found, return the original string\n",
        "        return input_string"
      ],
      "metadata": {
        "id": "b5YB3_ZHWrDI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_string(input_string):\n",
        "    # Replace newlines with a space to handle multi-line inputs as a single string\n",
        "    input_string = input_string.replace('\\n', ' ')\n",
        "\n",
        "    # Remove leading numbers followed by a dot and optional whitespace\n",
        "    input_string = re.sub(r'\\d+\\.\\s*', '', input_string)\n",
        "\n",
        "    # Remove all occurrences of * # - :\n",
        "    input_string = re.sub(r'[\\*#\\-:]', '', input_string)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    cleaned_string = ' '.join(input_string.split())\n",
        "\n",
        "    return cleaned_string"
      ],
      "metadata": {
        "id": "tiHjphUZIlXL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_keywords(input_string):\n",
        "    # Process the input string with spaCy NLP\n",
        "    doc = nlp(input_string)\n",
        "    # Extract named entities of type PERSON, TIME, and GPE (Geo-Political Entity)\n",
        "    #keywords = [ent.text for ent in doc.ents if ent.label_ in {'PERSON', 'TIME', 'GPE', 'ORG'}]\n",
        "    keywords = [ent.text for ent in doc.ents if ent.label_ in { 'PERSON','ORG'}]\n",
        "    # Join the keywords with a comma and return\n",
        "    return ', '.join(keywords)"
      ],
      "metadata": {
        "id": "RLMyP3DMnLPw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_empty_positions(input_list):\n",
        "    # Iterate over the list and replace empty positions with 'Not Mentioned'\n",
        "    return ['Not Mentioned' if not item else item for item in input_list]"
      ],
      "metadata": {
        "id": "YAtw73T5Z3u9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_Offensekeywords(input_string):\n",
        "    # Process the input string with spaCy NLP\n",
        "    doc = nlp(input_string)\n",
        "    # Extract named entities of type PERSON, TIME, and GPE (Geo-Political Entity)\n",
        "    #keywords = [ent.text for ent in doc.ents if ent.label_ in {'PERSON', 'TIME', 'GPE', 'ORG'}]\n",
        "    keywords = [ent.text for ent in doc.ents if ent.label_ in {'PERSON', 'ORG', 'GPE'}]\n",
        "    # Join the keywords with a comma and return\n",
        "    return ', '.join(keywords)"
      ],
      "metadata": {
        "id": "6EOdm2wvNIGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method 2\n",
        "def cos_similarity(sentence1,sentence2):\n",
        "   # Vectorize sentences\n",
        "   vectorizer = CountVectorizer().fit_transform([sentence1, sentence2])\n",
        "   vectors = vectorizer.toarray()\n",
        "\n",
        "   # Compute cosine similarity\n",
        "   cosine_sim = cosine_similarity(vectors[0].reshape(1, -1), vectors[1].reshape(1, -1))[0][0]\n",
        "\n",
        "   return cosine_sim\n",
        "\n",
        "#method 3\n",
        "def jaccard_similarity(sentence1, sentence2):\n",
        "    set1 = set(sentence1.split())\n",
        "    set2 = set(sentence2.split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0\n",
        "\n",
        "#method 4\n",
        "def bm25_similarity(sentence1,sentence2):\n",
        "    # Tokenize sentences\n",
        "    tokenized_sentence1 = list(jieba.cut(sentence1))\n",
        "    tokenized_sentence2 = list(jieba.cut(sentence2))\n",
        "    #tokenized_sentence1 = sentence1.lower().split()\n",
        "    #tokenized_sentence2 = sentence2.lower().split()\n",
        "\n",
        "    # Initialize BM25 model\n",
        "    bm25 = BM25Okapi([tokenized_sentence2])\n",
        "\n",
        "    # Compute BM25 score\n",
        "    bm25_score = bm25.get_scores(tokenized_sentence1)\n",
        "\n",
        "    return bm25_score\n",
        "\n",
        "#method 5\n",
        "def sorensen_dice_coefficient(sentence1, sentence2):\n",
        "    set1 = set(sentence1.split())\n",
        "    set2 = set(sentence2.split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    dice_coefficient = 2 * intersection / (len(set1) + len(set2))\n",
        "    return dice_coefficient\n",
        "\n",
        "#method 6\n",
        "def lev_similarity(sentence1,sentence2):\n",
        "   # Compute Levenshtein Distance\n",
        "   lev_distance = lev.distance(sentence1, sentence2)\n",
        "   # Normalize distance to a similarity score\n",
        "   lev_similarity = 1 - lev_distance / max(len(sentence1), len(sentence2))\n",
        "\n",
        "   return lev_similarity\n",
        "\n",
        "#method 8\n",
        "def calculate_bleu_score(reference_sentence, candidate_sentence):\n",
        "    \"\"\"\n",
        "    Calculate the BLEU score between two sentences.\n",
        "\n",
        "    Parameters:\n",
        "    reference_sentence (str): The reference sentence.\n",
        "    candidate_sentence (str): The candidate sentence.\n",
        "\n",
        "    Returns:\n",
        "    float: The BLEU score between the two sentences.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentences\n",
        "    reference_tokens = [reference_sentence.split()]\n",
        "    candidate_tokens = candidate_sentence.split()\n",
        "\n",
        "    # Smoothing function to handle cases with small sample size\n",
        "    smoothie = SmoothingFunction().method4\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "#method 9\n",
        "def calculate_rouge_score(reference_sentence, candidate_sentence):\n",
        "    \"\"\"\n",
        "    Calculate the ROUGE-1 score between two sentences.\n",
        "\n",
        "    Parameters:\n",
        "    reference_sentence (str): The reference sentence.\n",
        "    candidate_sentence (str): The candidate sentence.\n",
        "\n",
        "    Returns:\n",
        "    dict: The ROUGE-1 precision, recall, and F1 score.\n",
        "    \"\"\"\n",
        "    # Initialize the ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "\n",
        "    # Calculate the ROUGE score\n",
        "    scores = scorer.score(reference_sentence, candidate_sentence)\n",
        "\n",
        "    return scores['rouge1']\n",
        "\n",
        "#method 10\n",
        "def calculate_iou(reference_sentence, candidate_sentence):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) score between two sentences.\n",
        "\n",
        "    Parameters:\n",
        "    reference_sentence (str): The reference sentence.\n",
        "    candidate_sentence (str): The candidate sentence.\n",
        "\n",
        "    Returns:\n",
        "    float: The IoU score between the two sentences.\n",
        "    \"\"\"\n",
        "    # Tokenize the sentences into sets of words\n",
        "    reference_tokens = set(reference_sentence.split())\n",
        "    candidate_tokens = set(candidate_sentence.split())\n",
        "\n",
        "    # Handle edge case where both sentences are empty\n",
        "    if not reference_tokens and not candidate_tokens:\n",
        "        return 1.0\n",
        "\n",
        "    # Handle edge case where one of the sentences is empty\n",
        "    if not reference_tokens or not candidate_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate the intersection and union of the two sets\n",
        "    intersection = reference_tokens.intersection(candidate_tokens)\n",
        "    union = reference_tokens.union(candidate_tokens)\n",
        "\n",
        "    # Calculate the IoU score\n",
        "    iou_score = len(intersection) / len(union)\n",
        "\n",
        "    return iou_score\n",
        "\n",
        "#method 11\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalize text by removing extra spaces, converting to lowercase, and stripping leading/trailing spaces.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to normalize.\n",
        "\n",
        "    Returns:\n",
        "    str: The normalized text.\n",
        "    \"\"\"\n",
        "    text = text.strip().lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "def calculate_exact_match(reference_sentence, candidate_sentence):\n",
        "    \"\"\"\n",
        "    Calculate the Exact Match (EM) score between two sentences.\n",
        "\n",
        "    Parameters:\n",
        "    reference_sentence (str): The reference sentence.\n",
        "    candidate_sentence (str): The candidate sentence.\n",
        "\n",
        "    Returns:\n",
        "    int: The EM score (1 if sentences match exactly, 0 otherwise).\n",
        "    \"\"\"\n",
        "    # Normalize the sentences\n",
        "    reference_sentence = normalize_text(reference_sentence)\n",
        "    candidate_sentence = normalize_text(candidate_sentence)\n",
        "\n",
        "    # Calculate EM score\n",
        "    return 1 if reference_sentence == candidate_sentence else 0"
      ],
      "metadata": {
        "id": "amHEysO5oTUW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_data=pd.read_excel('drive/My Drive/Colab Notebooks/Spyglass165.xlsx')\n",
        "#test_data=pd.read_excel('drive/My Drive/Colab Notebooks/extracted_data.xlsx')\n",
        "test_data=pd.read_excel('drive/My Drive/Colab Notebooks/'+output_file_name)\n",
        "#label_data=label_data.fillna('Not Mentioned'\n",
        "#test_data['Vessel Name']\n",
        "label_data=label_data.fillna('Not Mentioned')\n",
        "test_data=test_data.fillna('Not Mentioned')\n",
        "#label_data.iloc[16,21]"
      ],
      "metadata": {
        "id": "xv3oKyE5-fKF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label_data['Crime Baskets for convergence']"
      ],
      "metadata": {
        "id": "gLjwJzXSlphA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_compare=min(len(test_data),len(label_data))\n",
        "compare_total=6\n",
        "score_mat=np.zeros((len_compare,compare_total))\n",
        "\n",
        "#different columns require different preprocessing since the structure of test_data and label_data has difference\n",
        "#VesselName, VesselFlag, OffenseInformation and GoodsOnboard are easier since structure of them in both files are simple and similar(just names or items)\n",
        "VesselName_cleaned=[]\n",
        "VesselFlag_cleaned=[]\n",
        "GoodsOnboard_cleaned=[]\n",
        "Offense_cleaned=[]\n",
        "Ownership_cleaned=[]\n",
        "Arrestinfromation_cleaned=[]\n",
        "\n",
        "VesselName_label=[]\n",
        "VesselFlag_label=[]\n",
        "GoodsOnboard_label=[]\n",
        "Offense_label=[]\n",
        "Ownership_label=[]\n",
        "Arrestinfromation_label=[]\n",
        "for i in range(len_compare):\n",
        "  VesselName_cleaned.append(clean_string(test_data['Vessel Name'][i]))\n",
        "  VesselFlag_cleaned.append(clean_string(test_data['Vessel Flag'][i]))\n",
        "  GoodsOnboard_cleaned.append(clean_string(test_data['Goods Onboard'][i]))\n",
        "  Offense_cleaned.append(clean_string(test_data['Offense information'][i]))\n",
        "  Ownership_cleaned.append(clean_string(test_data['Ownership Information'][i]))\n",
        "  Arrestinfromation_cleaned.append(clean_string(test_data['Arrest Information'][i]))\n",
        "\n",
        "  VesselName_label.append(label_data['Vessel Name'][i])\n",
        "  VesselFlag_label.append(label_data.iloc[i,8])\n",
        "  GoodsOnboard_label.append(label_data.iloc[i,21])\n",
        "  Offense_label.append(label_data['Offence information'][i])\n",
        "  Ownership_label.append(label_data['Owner name'][i])\n",
        "  Arrestinfromation_label.append(remove_before_colon(label_data['Crime Baskets for convergence'][i]+', arrested by '+label_data['Arresting authority'][i]))\n",
        "#Arrestinfromation_cleaned"
      ],
      "metadata": {
        "id": "T8vK71XRDUCe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ownership_cleaned"
      ],
      "metadata": {
        "id": "jg1kzJP2pe6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ownership_keywords=[]\n",
        "for i in range(len_compare):\n",
        "  Ownership_keywords.append(extract_keywords(Ownership_cleaned[i]))\n",
        "\n",
        "Ownership_keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_psxEF8mnxTW",
        "outputId": "1b768cca-63e5-470e-b24c-8bafd2e841bd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sajo Oyang',\n",
              " '',\n",
              " 'John Duncan, Jerry Ramsay',\n",
              " 'ALS Fisheries',\n",
              " 'Westfleet Fishing Limited']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Arrestinfromation_cleaned"
      ],
      "metadata": {
        "id": "JGLKwCNRMcHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Arrest_informationKeywords=[]\n",
        "#for i in range(len_compare):\n",
        "#  Arrest_informationKeywords.append(extract_Offensekeywords(Arrestinfromation_cleaned[i]))\n",
        "\n",
        "#Arrest_informationKeywords"
      ],
      "metadata": {
        "id": "ytDbiOPSMJW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_model = WebBertSimilarity(device='cpu', batch_size=10) #defaults to GPU prediction\n",
        "clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction"
      ],
      "metadata": {
        "id": "xLeIxiMIb34k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d650d7-6831-4f8b-deca-aa74374bc27a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model: web-bert-similarity from https://github.com/AndriyMulyar/semantic-text-similarity/releases/download/v1.0.0/web_bert_similarity.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405359924/405359924 [00:08<00:00, 46657615.18B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model: clinical-bert-similarity from https://github.com/AndriyMulyar/semantic-text-similarity/releases/download/v1.0.0/clinical_bert_similarity.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 401555686/401555686 [00:06<00:00, 57494715.12B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Arrestinfromation_cleaned"
      ],
      "metadata": {
        "id": "d29myvGCY47b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VesselName_cleaned=fill_empty_positions(VesselName_cleaned)\n",
        "VesselFlag_cleaned=fill_empty_positions(VesselFlag_cleaned)\n",
        "GoodsOnboard_cleaned=fill_empty_positions(GoodsOnboard_cleaned)\n",
        "Offense_cleaned=fill_empty_positions(Offense_cleaned)\n",
        "Ownership_cleaned=fill_empty_positions(Ownership_cleaned)\n",
        "Arrestinfromation_cleaned=fill_empty_positions(Arrestinfromation_cleaned)\n",
        "#unify empty contents that not mentioned\n",
        "\n",
        "Method_number=7\n",
        "if Method_number==1:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=web_model.predict([(VesselName_cleaned[i],VesselName_label[i])])\n",
        "    score_mat[i,1]=web_model.predict([(VesselFlag_cleaned[i],VesselFlag_label[i])])\n",
        "    score_mat[i,2]=web_model.predict([(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])])\n",
        "    score_mat[i,3]=web_model.predict([(Offense_cleaned[i],Offense_label[i])])\n",
        "    score_mat[i,4]=web_model.predict([(Ownership_keywords[i],Ownership_label[i])])\n",
        "    score_mat[i,5]=web_model.predict([(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])])\n",
        "\n",
        "elif Method_number==2:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=cos_similarity(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=cos_similarity(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=cos_similarity(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=cos_similarity(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=cos_similarity(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=cos_similarity(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==3:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=jaccard_similarity(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=jaccard_similarity(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=jaccard_similarity(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=jaccard_similarity(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=jaccard_similarity(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=jaccard_similarity(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==4:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=bm25_similarity(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=bm25_similarity(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=bm25_similarity(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=bm25_similarity(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=bm25_similarity(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=bm25_similarity(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==5:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=sorensen_dice_coefficient(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=sorensen_dice_coefficient(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=sorensen_dice_coefficient(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=sorensen_dice_coefficient(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=sorensen_dice_coefficient(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=sorensen_dice_coefficient(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==6:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=lev_similarity(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=lev_similarity(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=lev_similarity(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=lev_similarity(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=lev_similarity(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=lev_similarity(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==7:#combine as you like\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=calculate_exact_match(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=calculate_exact_match(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=calculate_iou(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=web_model.predict([(Offense_cleaned[i],Offense_label[i])])\n",
        "    score_mat[i,4]=calculate_bleu_score(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=web_model.predict([(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])])\n",
        "\n",
        "elif Method_number==8:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=calculate_bleu_score(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=calculate_bleu_score(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=calculate_bleu_score(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=calculate_bleu_score(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=calculate_bleu_score(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=calculate_bleu_score(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==9:\n",
        "  for i in range(len_compare):#choose precision\n",
        "    score_mat[i,0]=calculate_rouge_score(VesselName_cleaned[i],VesselName_label[i])[0]\n",
        "    score_mat[i,1]=calculate_rouge_score(VesselFlag_cleaned[i],VesselFlag_label[i])[0]\n",
        "    score_mat[i,2]=calculate_rouge_score(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])[0]\n",
        "    score_mat[i,3]=calculate_rouge_score(Offense_cleaned[i],Offense_label[i])[0]\n",
        "    score_mat[i,4]=calculate_rouge_score(Ownership_keywords[i],Ownership_label[i])[0]\n",
        "    score_mat[i,5]=calculate_rouge_score(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])[0]\n",
        "\n",
        "elif Method_number==10:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=calculate_iou(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=calculate_iou(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=calculate_iou(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=calculate_iou(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=calculate_iou(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=calculate_iou(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])\n",
        "\n",
        "elif Method_number==11:\n",
        "  for i in range(len_compare):\n",
        "    score_mat[i,0]=calculate_exact_match(VesselName_cleaned[i],VesselName_label[i])\n",
        "    score_mat[i,1]=calculate_exact_match(VesselFlag_cleaned[i],VesselFlag_label[i])\n",
        "    score_mat[i,2]=calculate_exact_match(GoodsOnboard_cleaned[i],GoodsOnboard_label[i])\n",
        "    score_mat[i,3]=calculate_exact_match(Offense_cleaned[i],Offense_label[i])\n",
        "    score_mat[i,4]=calculate_exact_match(Ownership_keywords[i],Ownership_label[i])\n",
        "    score_mat[i,5]=calculate_exact_match(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rISeC54AcmEX",
        "outputId": "03531419-e2ce-4e00-87be-5e045128f27a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-7b029cea13a7>:69: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  score_mat[i,3]=web_model.predict([(Offense_cleaned[i],Offense_label[i])])\n",
            "<ipython-input-27-7b029cea13a7>:71: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  score_mat[i,5]=web_model.predict([(Arrestinfromation_cleaned[i],Arrestinfromation_label[i])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#score_mat\n",
        "Final_score=sum(score_mat.mean(axis=1))/len_compare\n",
        "Final_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP7-Zqs6dQ8g",
        "outputId": "c310db06-5c70-4c31-8ec9-fef67a6bb708"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27896277819361004"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "fU8yzlj8BfmD"
      }
    }
  ]
}